{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cd8bb1",
   "metadata": {},
   "source": [
    "Exactly like addsenseboarddata from Straight Run:\n",
    "# addsenseboarddata – Enrich `all_data.csv` with SenseBoard Loadcell Signals\n",
    "\n",
    "This notebook augments the unified dataset (`all_data.csv`) with high-frequency **SenseBoard** measurements (loadcells, moments, and positions) by time-aligning SenseBoard logs to SenseBoard boat rows.\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "1. **Load base dataset**\n",
    "   - Reads `all_data.csv`, parses `ISODateTimeUTC`, and tags each row with an internal `__original_id__` to preserve identity during merges.\n",
    "\n",
    "2. **Split SenseBoard vs. other boats**\n",
    "   - Separates rows where `boat_name == \"SenseBoard\"` from all other boats.  \n",
    "   - Only SenseBoard rows are candidates for enrichment with loadcell data.\n",
    "\n",
    "3. **Scan SenseBoard logs**\n",
    "   - For each session folder under `../Data_Sailnjord/Straight_lines/<DATE>/senseboard_log/`, loads the Excel file `SenseBoard_log_modified*.xlsx`.\n",
    "   - Keeps relevant columns when present:  \n",
    "     `Timestamp GPS`, `Latitude`, `Longitude`, `Euler_X (deg)`, `Euler_Y (deg)`,  \n",
    "     `LoadCell_1..6`, `F_front`, `F_back`, `M_tot_X`, `M_tot_Y`,  \n",
    "     `M_front_X`, `M_front_Y`, `M_back_X`, `M_back_Y`, `P_front_X`, `P_front_Y`, `P_back_X`, `P_back_Y`.\n",
    "\n",
    "4. **Clean and normalize SenseBoard timestamps**\n",
    "   - Parses `Timestamp GPS` with microsecond precision and localizes to UTC.\n",
    "   - Converts numeric columns; drops invalid timestamps.\n",
    "   - **Spreads duplicate timestamps** using `spread_duplicate_timestamps(...)` to micro-jitter duplicates (default 100 ms spacing) so that as-of merges are well-defined.\n",
    "\n",
    "5. **Time alignment (`merge_asof`)**\n",
    "   - For SenseBoard rows whose `ISODateTimeUTC` falls within the log’s time range, performs nearest-neighbor alignment:\n",
    "     - `left_on = ISODateTimeUTC` (from `all_data.csv`)\n",
    "     - `right_on = Timestamp GPS` (from SenseBoard log)\n",
    "     - `tolerance = 200 ms`, `direction = 'nearest'`\n",
    "   - Collects enriched chunks across all sessions.\n",
    "\n",
    "6. **Post-merge integrity checks**\n",
    "   - Drops duplicate `__original_id__` after enrichment to keep a single enriched record per original row.\n",
    "   - Recombines enriched SenseBoard rows with untouched non-SenseBoard rows.\n",
    "   - Restores any missing mirrored rows (same boat/opponent/timestamp) by comparing against the expected pairs from the original dataset and re-inserting any lost entries.\n",
    "\n",
    "7. **Output**\n",
    "   - Writes the final, time-aligned dataset to **`all_data_enriched.csv`** (drops `__original_id__`).\n",
    "\n",
    "## Notes\n",
    "- The enrichment is **non-destructive**: if no SenseBoard log matches a time span, original rows are preserved unmodified.\n",
    "- The 200 ms tolerance and 100 ms de-duplication spacing are configurable if your logs’ timing characteristics change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1defc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T13:15:45.614911Z",
     "iopub.status.busy": "2025-09-13T13:15:45.614911Z",
     "iopub.status.idle": "2025-09-13T13:16:46.364049Z",
     "shell.execute_reply": "2025-09-13T13:16:46.363041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 08_06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 11_06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enriched dataset saved to all_data_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "\n",
    "def spread_duplicate_timestamps(df, timestamp_col=\"Timestamp GPS\", interval_ms=100):\n",
    "    new_rows = []\n",
    "    for ts, group in df.groupby(timestamp_col):\n",
    "        if len(group) == 1:\n",
    "            new_rows.append(group)\n",
    "        else:\n",
    "            deltas = pd.to_timedelta(np.linspace(0, interval_ms, len(group), endpoint=False), unit=\"ms\")\n",
    "            group = group.copy()\n",
    "            group[timestamp_col] = ts + deltas\n",
    "            new_rows.append(group)\n",
    "    return pd.concat(new_rows).sort_values(timestamp_col).reset_index(drop=True)\n",
    "\n",
    "# === Step 1: Load data ===\n",
    "all_data = pd.read_csv(\"all_data.csv\")\n",
    "all_data[\"ISODateTimeUTC\"] = pd.to_datetime(all_data[\"ISODateTimeUTC\"], errors=\"coerce\")\n",
    "all_data[\"__original_id__\"] = all_data.index  # Track original\n",
    "\n",
    "# === Step 2: Split rows ===\n",
    "senseboard_rows = all_data[all_data[\"boat_name\"] == \"SenseBoard\"].copy()\n",
    "other_rows = all_data[all_data[\"boat_name\"] != \"SenseBoard\"].copy()\n",
    "\n",
    "# === Step 3: Prepare enrichment ===\n",
    "root_dir = Path(\"../Data_Sailnjord/Maneuvers/\")  # Adjust path as necessary\n",
    "merged_parts = []\n",
    "\n",
    "cols_to_keep = [\n",
    "    \"Timestamp GPS\", \"Latitude\", \"Longitude\", \"Euler_X (deg)\", \"Euler_Y (deg)\",\n",
    "    \"LoadCell_1\", \"LoadCell_2\", \"LoadCell_3\", \"LoadCell_4\", \"LoadCell_5\", \"LoadCell_6\",\n",
    "    \"F_front\", \"F_back\",\n",
    "    \"M_tot_X\", \"M_tot_Y\",\n",
    "    \"M_front_X\", \"M_front_Y\",\n",
    "    \"M_back_X\", \"M_back_Y\",\n",
    "    \"P_front_X\", \"P_front_Y\",\n",
    "    \"P_back_X\", \"P_back_Y\"\n",
    "]\n",
    "\n",
    "for subfolder in root_dir.iterdir():\n",
    "    if not subfolder.is_dir():\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing folder: {subfolder.name}\")\n",
    "    log_dir = subfolder / \"senseboard_log\"\n",
    "    senseboard_file = next(log_dir.glob(\"SenseBoard_log_modified*.xlsx\"), None) if log_dir.exists() else None\n",
    "    if senseboard_file is None:\n",
    "        print(f\"No SenseBoard file in {subfolder.name}\")\n",
    "        continue\n",
    "\n",
    "    sb_data = pd.read_excel(senseboard_file, dtype=str, engine=\"openpyxl\")\n",
    "    sb_data.columns = sb_data.columns.str.strip()\n",
    "\n",
    "    if \"Timestamp GPS\" not in sb_data.columns:\n",
    "        raise ValueError(\"Missing 'Timestamp GPS'\")\n",
    "\n",
    "    sb_data[\"Timestamp GPS\"] = pd.to_datetime(sb_data[\"Timestamp GPS\"], format=\"%Y-%m-%d %H:%M:%S.%f\", errors=\"coerce\")\n",
    "    sb_data = sb_data.dropna(subset=[\"Timestamp GPS\"])\n",
    "    sb_data[\"Timestamp GPS\"] = sb_data[\"Timestamp GPS\"].dt.tz_localize(\"UTC\")\n",
    "\n",
    "    for col in sb_data.columns:\n",
    "        if col != \"Timestamp GPS\":\n",
    "            sb_data[col] = pd.to_numeric(sb_data[col], errors=\"coerce\")\n",
    "\n",
    "    sb_data = spread_duplicate_timestamps(sb_data, interval_ms=100)\n",
    "    sb_data = sb_data[[col for col in cols_to_keep if col in sb_data.columns]]\n",
    "\n",
    "    start_time, end_time = sb_data[\"Timestamp GPS\"].min(), sb_data[\"Timestamp GPS\"].max()\n",
    "\n",
    "    sb_rows_sub = senseboard_rows[\n",
    "        (senseboard_rows[\"ISODateTimeUTC\"] >= start_time) &\n",
    "        (senseboard_rows[\"ISODateTimeUTC\"] <= end_time)\n",
    "    ].copy()\n",
    "\n",
    "    if sb_rows_sub.empty:\n",
    "        print(f\"No matching SenseBoard rows in {subfolder.name}\")\n",
    "        continue\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        sb_rows_sub.sort_values(\"ISODateTimeUTC\"),\n",
    "        sb_data.sort_values(\"Timestamp GPS\"),\n",
    "        left_on=\"ISODateTimeUTC\",\n",
    "        right_on=\"Timestamp GPS\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(\"200ms\")  # Increased from 50ms\n",
    "    )\n",
    "\n",
    "    merged_parts.append(merged)\n",
    "\n",
    "# === Step 4: Merge enriched data ===\n",
    "if merged_parts:\n",
    "    full_merged = pd.concat(merged_parts, ignore_index=True)\n",
    "else:\n",
    "    full_merged = senseboard_rows.copy()\n",
    "\n",
    "full_merged = full_merged.drop_duplicates(subset=[\"__original_id__\"])\n",
    "\n",
    "# === Step 5: Combine with untouched rows ===\n",
    "final = pd.concat([full_merged, other_rows], ignore_index=True)\n",
    "final = final.sort_values(\"ISODateTimeUTC\")\n",
    "\n",
    "# === Step 6: Restore missing mirrored rows ===\n",
    "expected_pairs = all_data[[\"boat_name\", \"ISODateTimeUTC\", \"__original_id__\"]]\n",
    "actual_pairs = final[[\"boat_name\", \"ISODateTimeUTC\"]].copy()\n",
    "\n",
    "merged_check = pd.merge(\n",
    "    expected_pairs,\n",
    "    actual_pairs,\n",
    "    on=[\"boat_name\", \"ISODateTimeUTC\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "missing = merged_check.query(\"_merge == 'left_only'\")\n",
    "recovered = pd.merge(\n",
    "    missing[[\"__original_id__\"]],\n",
    "    all_data,\n",
    "    on=\"__original_id__\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "final = pd.concat([final, recovered], ignore_index=True).sort_values(\"ISODateTimeUTC\")\n",
    "\n",
    "# === Step 7: Save output ===\n",
    "final.drop(columns=\"__original_id__\", errors=\"ignore\").to_csv(\"all_data_enriched.csv\", index=False)\n",
    "print(\"✅ Enriched dataset saved to all_data_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d862976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T13:16:46.368049Z",
     "iopub.status.busy": "2025-09-13T13:16:46.367101Z",
     "iopub.status.idle": "2025-09-13T13:16:46.374051Z",
     "shell.execute_reply": "2025-09-13T13:16:46.374051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35314\n",
      "35314\n"
     ]
    }
   ],
   "source": [
    "print(len(final))\n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d920c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sail2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
