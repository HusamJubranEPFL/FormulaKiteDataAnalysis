{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db1defc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T13:37:47.752810Z",
     "iopub.status.busy": "2025-07-29T13:37:47.751429Z",
     "iopub.status.idle": "2025-07-29T13:40:57.387864Z",
     "shell.execute_reply": "2025-07-29T13:40:57.387422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 08_06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 11_06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enriched dataset saved to all_data_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "\n",
    "def spread_duplicate_timestamps(df, timestamp_col=\"Timestamp GPS\", interval_ms=100):\n",
    "    new_rows = []\n",
    "    for ts, group in df.groupby(timestamp_col):\n",
    "        if len(group) == 1:\n",
    "            new_rows.append(group)\n",
    "        else:\n",
    "            deltas = pd.to_timedelta(np.linspace(0, interval_ms, len(group), endpoint=False), unit=\"ms\")\n",
    "            group = group.copy()\n",
    "            group[timestamp_col] = ts + deltas\n",
    "            new_rows.append(group)\n",
    "    return pd.concat(new_rows).sort_values(timestamp_col).reset_index(drop=True)\n",
    "\n",
    "# === Step 1: Load data ===\n",
    "all_data = pd.read_csv(\"all_data.csv\")\n",
    "all_data[\"ISODateTimeUTC\"] = pd.to_datetime(all_data[\"ISODateTimeUTC\"], errors=\"coerce\")\n",
    "all_data[\"__original_id__\"] = all_data.index  # Track original\n",
    "\n",
    "# === Step 2: Split rows ===\n",
    "senseboard_rows = all_data[all_data[\"boat_name\"] == \"SenseBoard\"].copy()\n",
    "other_rows = all_data[all_data[\"boat_name\"] != \"SenseBoard\"].copy()\n",
    "\n",
    "# === Step 3: Prepare enrichment ===\n",
    "root_dir = Path(\"../Data_Sailnjord/Maneuvers/\")  # Adjust path as necessary\n",
    "merged_parts = []\n",
    "\n",
    "cols_to_keep = [\n",
    "    \"Timestamp GPS\", \"Latitude\", \"Longitude\", \"Euler_X (deg)\", \"Euler_Y (deg)\",\n",
    "    \"LoadCell_1\", \"LoadCell_2\", \"LoadCell_3\", \"LoadCell_4\", \"LoadCell_5\", \"LoadCell_6\",\n",
    "    \"F_front\", \"F_back\",\n",
    "    \"M_tot_X\", \"M_tot_Y\",\n",
    "    \"M_front_X\", \"M_front_Y\",\n",
    "    \"M_back_X\", \"M_back_Y\",\n",
    "    \"P_front_X\", \"P_front_Y\",\n",
    "    \"P_back_X\", \"P_back_Y\"\n",
    "]\n",
    "\n",
    "for subfolder in root_dir.iterdir():\n",
    "    if not subfolder.is_dir():\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing folder: {subfolder.name}\")\n",
    "    log_dir = subfolder / \"senseboard_log\"\n",
    "    senseboard_file = next(log_dir.glob(\"SenseBoard_log_modified*.xlsx\"), None) if log_dir.exists() else None\n",
    "    if senseboard_file is None:\n",
    "        print(f\"No SenseBoard file in {subfolder.name}\")\n",
    "        continue\n",
    "\n",
    "    sb_data = pd.read_excel(senseboard_file, dtype=str, engine=\"openpyxl\")\n",
    "    sb_data.columns = sb_data.columns.str.strip()\n",
    "\n",
    "    if \"Timestamp GPS\" not in sb_data.columns:\n",
    "        raise ValueError(\"Missing 'Timestamp GPS'\")\n",
    "\n",
    "    sb_data[\"Timestamp GPS\"] = pd.to_datetime(sb_data[\"Timestamp GPS\"], format=\"%Y-%m-%d %H:%M:%S.%f\", errors=\"coerce\")\n",
    "    sb_data = sb_data.dropna(subset=[\"Timestamp GPS\"])\n",
    "    sb_data[\"Timestamp GPS\"] = sb_data[\"Timestamp GPS\"].dt.tz_localize(\"UTC\")\n",
    "\n",
    "    for col in sb_data.columns:\n",
    "        if col != \"Timestamp GPS\":\n",
    "            sb_data[col] = pd.to_numeric(sb_data[col], errors=\"coerce\")\n",
    "\n",
    "    sb_data = spread_duplicate_timestamps(sb_data, interval_ms=100)\n",
    "    sb_data = sb_data[[col for col in cols_to_keep if col in sb_data.columns]]\n",
    "\n",
    "    start_time, end_time = sb_data[\"Timestamp GPS\"].min(), sb_data[\"Timestamp GPS\"].max()\n",
    "\n",
    "    sb_rows_sub = senseboard_rows[\n",
    "        (senseboard_rows[\"ISODateTimeUTC\"] >= start_time) &\n",
    "        (senseboard_rows[\"ISODateTimeUTC\"] <= end_time)\n",
    "    ].copy()\n",
    "\n",
    "    if sb_rows_sub.empty:\n",
    "        print(f\"No matching SenseBoard rows in {subfolder.name}\")\n",
    "        continue\n",
    "\n",
    "    merged = pd.merge_asof(\n",
    "        sb_rows_sub.sort_values(\"ISODateTimeUTC\"),\n",
    "        sb_data.sort_values(\"Timestamp GPS\"),\n",
    "        left_on=\"ISODateTimeUTC\",\n",
    "        right_on=\"Timestamp GPS\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(\"200ms\")  # Increased from 50ms\n",
    "    )\n",
    "\n",
    "    merged_parts.append(merged)\n",
    "\n",
    "# === Step 4: Merge enriched data ===\n",
    "if merged_parts:\n",
    "    full_merged = pd.concat(merged_parts, ignore_index=True)\n",
    "else:\n",
    "    full_merged = senseboard_rows.copy()\n",
    "\n",
    "full_merged = full_merged.drop_duplicates(subset=[\"__original_id__\"])\n",
    "\n",
    "# === Step 5: Combine with untouched rows ===\n",
    "final = pd.concat([full_merged, other_rows], ignore_index=True)\n",
    "final = final.sort_values(\"ISODateTimeUTC\")\n",
    "\n",
    "# === Step 6: Restore missing mirrored rows ===\n",
    "expected_pairs = all_data[[\"boat_name\", \"ISODateTimeUTC\", \"__original_id__\"]]\n",
    "actual_pairs = final[[\"boat_name\", \"ISODateTimeUTC\"]].copy()\n",
    "\n",
    "merged_check = pd.merge(\n",
    "    expected_pairs,\n",
    "    actual_pairs,\n",
    "    on=[\"boat_name\", \"ISODateTimeUTC\"],\n",
    "    how=\"left\",\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "missing = merged_check.query(\"_merge == 'left_only'\")\n",
    "recovered = pd.merge(\n",
    "    missing[[\"__original_id__\"]],\n",
    "    all_data,\n",
    "    on=\"__original_id__\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "final = pd.concat([final, recovered], ignore_index=True).sort_values(\"ISODateTimeUTC\")\n",
    "\n",
    "# === Step 7: Save output ===\n",
    "final.drop(columns=\"__original_id__\", errors=\"ignore\").to_csv(\"all_data_enriched.csv\", index=False)\n",
    "print(\"✅ Enriched dataset saved to all_data_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d862976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T13:40:57.393723Z",
     "iopub.status.busy": "2025-07-29T13:40:57.387864Z",
     "iopub.status.idle": "2025-07-29T13:40:57.398200Z",
     "shell.execute_reply": "2025-07-29T13:40:57.398200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40684\n",
      "40684\n"
     ]
    }
   ],
   "source": [
    "print(len(final))\n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d920c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada_exam_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
