{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014c8c94",
   "metadata": {},
   "source": [
    "# AddInfoToSummary â€“ Enrichment of Straight Line Summary with Interview Data\n",
    "\n",
    "This notebook takes the initial straight line summary (`summary.json`, produced by `MainCOG.ipynb`) and **enriches it with rider and equipment information** extracted from interview files.\n",
    "\n",
    "### Workflow\n",
    "1. **Load interview data**  \n",
    "   - Scans each sessionâ€™s `Interview and equipment` folder.  \n",
    "   - Reads `.xlsx` files containing rider and equipment information.  \n",
    "   - Standardizes rider names (e.g., `Gian` â†’ `Gian Stragiotti`, `Karl` â†’ `Karl Maeder`).  \n",
    "\n",
    "2. **Match interview data with runs**  \n",
    "   - For each run and each leg (upwind or downwind) listed in `summary.json`, the notebook matches corresponding interview entries based on:  \n",
    "     - Rider name  \n",
    "     - Run number  \n",
    "     - Leg index  \n",
    "     - (Optionally) closest timestamp to the interval start time.  \n",
    "\n",
    "3. **Extract and map equipment information**  \n",
    "   - **Total weight** of the rider and gear.  \n",
    "   - **Master leeward** configuration.  \n",
    "   - **Mast brand** (mapped from codes: `0 â†’ Levi`, `1 â†’ Chub`).  \n",
    "\n",
    "4. **Update summary**  \n",
    "   - Each interval in the summary is enriched with additional fields for both boats:  \n",
    "     - `boatX_total_weight`  \n",
    "     - `boatX_master_leeward`  \n",
    "     - `boatX_mast_brand`  \n",
    "\n",
    "5. **Output**  \n",
    "   - Produces a new file: `summary_enriched.json`.  \n",
    "   - This enriched summary is used as input for subsequent data merging (`merge_all.ipynb`) and analyses.\n",
    "\n",
    "This notebook is the **second step of the pipeline**, connecting raw interval detection with contextual rider/equipment data from interviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173cfbdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:40:34.397515Z",
     "iopub.status.busy": "2025-09-13T12:40:34.397515Z",
     "iopub.status.idle": "2025-09-13T12:40:34.931793Z",
     "shell.execute_reply": "2025-09-13T12:40:34.931793Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9deb84a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:40:34.931793Z",
     "iopub.status.busy": "2025-09-13T12:40:34.931793Z",
     "iopub.status.idle": "2025-09-13T12:40:34.945379Z",
     "shell.execute_reply": "2025-09-13T12:40:34.945379Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration des chemins\n",
    "base_dir = \"../Data_Sailnjord/Straight_lines\"\n",
    "summary_file = \"summary.json\"\n",
    "output_file = \"summary_enriched.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e67ded2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:40:34.945379Z",
     "iopub.status.busy": "2025-09-13T12:40:34.945379Z",
     "iopub.status.idle": "2025-09-13T12:40:34.958594Z",
     "shell.execute_reply": "2025-09-13T12:40:34.958594Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_interview_data(interview_dir):\n",
    "    name_map = {\n",
    "        \"Gian\": \"Gian Stragiotti\",\n",
    "        \"Karl\": \"Karl Maeder\",\n",
    "        \"SenseBoard\": \"SenseBoard\"\n",
    "    }\n",
    "\n",
    "    dfs = []\n",
    "    for file in os.listdir(interview_dir):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            key = file.replace(\"Interview \", \"\").replace(\".xlsx\", \"\").split()[0]\n",
    "            name = name_map.get(key, key)\n",
    "            df = pd.read_excel(os.path.join(interview_dir, file))\n",
    "            df[\"Name\"] = name\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "def get_boat_info(df, boat_name, run_idx, leg_idx, interval_start_time=None):\n",
    "    candidates = df[\n",
    "        (df[\"Name\"].str.contains(boat_name, case=False)) &\n",
    "        (df[\"Run\"] == run_idx + 1) &\n",
    "        (df[\"Leg U=1, D=2\"] == leg_idx + 1)\n",
    "    ].copy()\n",
    "\n",
    "    if candidates.empty:\n",
    "        return {\n",
    "            \"total_weight\": None,\n",
    "            \"master_leeward\": None,\n",
    "            \"mast_brand\": None\n",
    "        }\n",
    "\n",
    "    if interval_start_time and \"Timestamp\" in candidates.columns:\n",
    "        candidates[\"abs_diff\"] = (candidates[\"Timestamp\"] - interval_start_time).abs()\n",
    "        candidates = candidates.sort_values(\"abs_diff\")\n",
    "\n",
    "    row = candidates.iloc[0]\n",
    "\n",
    "    # Mapping 0/1 vers \"Levi\"/\"Chub\"\n",
    "    brand_map = {0: \"Levi\", 1: \"Chub\"}\n",
    "    raw_brand = row.get(\"Mast brand (0=Levi,1=Chub)\", None)\n",
    "    mast_brand = brand_map.get(int(raw_brand)) if pd.notnull(raw_brand) else None\n",
    "\n",
    "    return {\n",
    "        \"total_weight\": row.get(\"Total weight\", None),\n",
    "        \"master_leeward\": bool(row.get(\"Master leeward (1)\", False)),\n",
    "        \"mast_brand\": mast_brand\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bee3b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:40:34.958594Z",
     "iopub.status.busy": "2025-09-13T12:40:34.958594Z",
     "iopub.status.idle": "2025-09-13T12:40:36.177541Z",
     "shell.execute_reply": "2025-09-13T12:40:36.176625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RÃ©sumÃ© enrichi sauvÃ© dans summary_enriched.json\n"
     ]
    }
   ],
   "source": [
    "with open(summary_file, \"r\") as f:\n",
    "    summary_data = json.load(f)\n",
    "\n",
    "for date_folder in sorted(os.listdir(base_dir)):\n",
    "    date_path = os.path.join(base_dir, date_folder)\n",
    "    if not os.path.isdir(date_path) or \"Interview\" in date_folder:\n",
    "        continue\n",
    "\n",
    "    interview_dir = os.path.join(date_path, \"Interview and equipment\")\n",
    "    if not os.path.exists(interview_dir):\n",
    "        print(f\"ðŸ“‚ Dossier dâ€™interview manquant pour {date_folder}\")\n",
    "        continue\n",
    "\n",
    "    interview_df = load_interview_data(interview_dir)\n",
    "\n",
    "    for run in summary_data:\n",
    "        if run[\"run\"].startswith(date_folder):\n",
    "            try:\n",
    "                run_number = int(run[\"run\"].split(\"_Run\")[1]) - 1\n",
    "            except:\n",
    "                continue\n",
    "            for leg_idx, interval in enumerate(run[\"intervals\"]):\n",
    "                for b in [1, 2]:\n",
    "                    boat = interval.get(f\"boat{b}_name\", \"\")\n",
    "                    info = get_boat_info(interview_df, boat, run_number, leg_idx)\n",
    "                    interval[f\"boat{b}_total_weight\"] = info[\"total_weight\"]\n",
    "                    interval[f\"boat{b}_master_leeward\"] = info[\"master_leeward\"]\n",
    "                    interval[f\"boat{b}_mast_brand\"] = info[\"mast_brand\"]\n",
    "\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(summary_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… RÃ©sumÃ© enrichi sauvÃ© dans {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sail2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
