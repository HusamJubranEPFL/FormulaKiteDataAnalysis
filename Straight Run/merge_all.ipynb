{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d66b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from cog_analysis import load_boat_data\n",
    "from report_fct import filter_interval\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from cog_analysis import analyze_session, load_boat_data\n",
    "from report_fct import filter_interval  # si tu as déjà cette fonction ailleurs\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from cog_analysis import load_boat_data\n",
    "from report_fct import filter_interval  # or define it if needed\n",
    "\n",
    "\"\"\"\n",
    "def build_csv_from_summary(summary_path, data_root, output_csv=\"all_data.csv\"):\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for run_entry in summary:\n",
    "        run_name = run_entry[\"run\"]\n",
    "        intervals = run_entry[\"intervals\"]\n",
    "\n",
    "        # Path where CSVs are stored: we assume date/run_name structure\n",
    "        run_path = None\n",
    "        for root, dirs, files in os.walk(data_root):\n",
    "            if os.path.basename(root) == run_name:\n",
    "                run_path = root\n",
    "                break\n",
    "\n",
    "        if not run_path:\n",
    "            print(f\"⚠️ Run folder not found for: {run_name}\")\n",
    "            continue\n",
    "\n",
    "        csv_files = [f for f in os.listdir(run_path) if f.endswith(\".csv\")]\n",
    "        if len(csv_files) != 2:\n",
    "            print(f\"⚠️ Skipping {run_name}: expected 2 CSVs, found {len(csv_files)}\")\n",
    "            continue\n",
    "\n",
    "        csv_paths = [os.path.join(run_path, f) for f in csv_files]\n",
    "\n",
    "        try:\n",
    "            df1, df2, name1, name2 = load_boat_data(csv_paths[0], csv_paths[1])\n",
    "            if df1.empty or df2.empty:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CSVs for {run_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for i, interval in enumerate(intervals):\n",
    "            start, end = interval[\"start_time\"], interval[\"end_time\"]\n",
    "            if end - start < 30:  # Skip intervals shorter than 30 seconds\n",
    "                print(f\"⚠️ Skipping interval {i + 1} for {run_name}: duration < 30 seconds\")\n",
    "                continue\n",
    "\n",
    "            df1_clip = filter_interval(df1, start, end)\n",
    "            df2_clip = filter_interval(df2, start, end)\n",
    "            if df1_clip.empty or df2_clip.empty:\n",
    "                print(f\"⚠️ Skipping interval {i + 1} for {run_name}: no data in interval\")\n",
    "                continue\n",
    "\n",
    "            for df_clip, prefix in [(df1_clip, \"boat1\"), (df2_clip, \"boat2\")]:\n",
    "                df = df_clip.copy()\n",
    "                df[\"run\"] = run_name\n",
    "                df[\"interval_id\"] = i + 1\n",
    "                df[\"boat_name\"] = interval.get(f\"{prefix}_name\", \"\")\n",
    "                df[\"boat_role\"] = \"master\" if interval.get(f\"{prefix}_master_leeward\", False) else \"slave\"\n",
    "                # df[\"avg_SOG_interval\"] = interval.get(f\"avg_SOG_{prefix}\", None)\n",
    "                # df[\"SOG_variation_interval\"] = interval.get(f\"SOG_variation_{prefix}\", None)\n",
    "                # df[\"avg_TWA_interval\"] = interval.get(f\"avg TWA {prefix.replace('boat', 'boat ')}\", None)\n",
    "                df[\"boat_weight\"] = interval.get(f\"{prefix}_total_weight\", None)\n",
    "                df[\"interval_duration\"] = interval.get(\"duration\", None)\n",
    "                # df[\"stability_score\"] = interval.get(\"stability_score\", None)\n",
    "                all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"❌ No valid data found.\")\n",
    "        return\n",
    "\n",
    "    df_global = pd.concat(all_rows, ignore_index=True)\n",
    "    df_global.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Global CSV saved to: {output_csv}\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cog_analysis import load_boat_data\n",
    "from report_fct import filter_interval\n",
    "from report_fct import compute_directional_gain\n",
    "\n",
    "def build_csv_from_summary(summary_path, data_root, output_csv=\"all_data.csv\"):\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for run_entry in summary:\n",
    "        run_name = run_entry[\"run\"]\n",
    "        intervals = run_entry[\"intervals\"]\n",
    "\n",
    "        run_path = None\n",
    "        for root, dirs, files in os.walk(data_root):\n",
    "            if os.path.basename(root) == run_name:\n",
    "                run_path = root\n",
    "                break\n",
    "\n",
    "        if not run_path:\n",
    "            print(f\"⚠️ Run folder not found for: {run_name}\")\n",
    "            continue\n",
    "\n",
    "        csv_files = [f for f in os.listdir(run_path) if f.endswith(\".csv\")]\n",
    "        if len(csv_files) != 2:\n",
    "            print(f\"⚠️ Skipping {run_name}: expected 2 CSVs, found {len(csv_files)}\")\n",
    "            continue\n",
    "\n",
    "        csv_paths = [os.path.join(run_path, f) for f in csv_files]\n",
    "\n",
    "        try:\n",
    "            df1, df2, name1, name2 = load_boat_data(csv_paths[0], csv_paths[1])\n",
    "            if df1.empty or df2.empty:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CSVs for {run_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for i, interval in enumerate(intervals):\n",
    "            start, end = interval[\"start_time\"], interval[\"end_time\"]\n",
    "            if end - start < 30:\n",
    "                print(f\"⚠️ Skipping interval {i + 1} for {run_name}: duration < 30 seconds\")\n",
    "                continue\n",
    "\n",
    "            df1_clip = filter_interval(df1, start, end)\n",
    "            df2_clip = filter_interval(df2, start, end)\n",
    "            if df1_clip.empty or df2_clip.empty:\n",
    "                print(f\"⚠️ Skipping interval {i + 1} for {run_name}: no data in interval\")\n",
    "                continue\n",
    "\n",
    "            # Déterminer le rôle de chaque bateau\n",
    "            master_df, slave_df = (df1_clip, df2_clip)\n",
    "            if not interval.get(\"boat1_master_leeward\", False):\n",
    "                master_df, slave_df = df2_clip, df1_clip\n",
    "\n",
    "            for df_clip, prefix in [(df1_clip, \"boat1\"), (df2_clip, \"boat2\")]:\n",
    "                df = df_clip.copy()\n",
    "                df[\"run\"] = run_name\n",
    "                df[\"interval_id\"] = i + 1\n",
    "                df[\"boat_name\"] = interval.get(f\"{prefix}_name\", \"\")\n",
    "                df[\"boat_role\"] = \"master\" if interval.get(f\"{prefix}_master_leeward\", False) else \"slave\"\n",
    "                df[\"boat_weight\"] = interval.get(f\"{prefix}_total_weight\", None)\n",
    "                df[\"interval_duration\"] = interval.get(\"duration\", None)\n",
    "\n",
    "                # Calcul des gains à chaque timestamp\n",
    "                gain_forward = []\n",
    "                gain_lateral = []\n",
    "                gain_vmg = []\n",
    "\n",
    "                for t in df[\"SecondsSince1970\"]:\n",
    "                    m_clip = master_df[master_df[\"SecondsSince1970\"] <= t]\n",
    "                    s_clip = slave_df[slave_df[\"SecondsSince1970\"] <= t]\n",
    "                    gain_df = compute_directional_gain(m_clip, s_clip)\n",
    "\n",
    "                    if gain_df.empty:\n",
    "                        gain_forward.append(np.nan)\n",
    "                        gain_lateral.append(np.nan)\n",
    "                        gain_vmg.append(np.nan)\n",
    "                    else:\n",
    "                        gain_forward.append(gain_df.loc[\"Total Gain\", \"Forward\"])\n",
    "                        gain_lateral.append(gain_df.loc[\"Total Gain\", \"Lateral\"])\n",
    "                        gain_vmg.append(gain_df.loc[\"Total Gain\", \"VMG\"])\n",
    "\n",
    "                df[\"gain_forward\"] = gain_forward\n",
    "                df[\"gain_lateral\"] = gain_lateral\n",
    "                df[\"gain_vmg\"] = gain_vmg\n",
    "\n",
    "                all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"❌ No valid data found.\")\n",
    "        return\n",
    "\n",
    "    df_global = pd.concat(all_rows, ignore_index=True)\n",
    "    df_global.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Global CSV saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dbb311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Global CSV saved to: all_data.csv\n"
     ]
    }
   ],
   "source": [
    "build_csv_from_summary(\n",
    "    summary_path=\"summary_enriched.json\",\n",
    "    data_root=\"../Data_Sailnjord/Straight_lines\",\n",
    "    output_csv=\"all_data.csv\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada_exam_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
