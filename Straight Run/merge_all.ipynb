{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ceec476",
   "metadata": {},
   "source": [
    "# merge_all – Build Unified Dataset and Compute Run-Level Metrics\n",
    "\n",
    "This notebook ingests the enriched straight-line intervals (from `summary_enriched.json`) together with the original CSV logs of each run, and produces a **row-level, analysis-ready dataset**. It relies on helper utilities implemented in **`report_fct.py`** and **`cog_analysis.py`**.\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "1. **Load run intervals**\n",
    "   - Reads `summary_enriched.json` and maps each `run` to its list of intervals.\n",
    "   - Locates the matching run folder under the data root and loads the two boat CSVs.\n",
    "\n",
    "2. **Clip data to intervals**\n",
    "   - For every interval with sufficient duration (≥ 30 s), slices each boat’s time series to `[start_time, end_time]` using `filter_interval(...)` from `report_fct.py`.\n",
    "\n",
    "3. **Assign roles and metadata**\n",
    "   - Determines **master/slave** roles from `*_master_leeward` flags in the summary.\n",
    "   - Attaches interval/rider/equipment metadata to each row:\n",
    "     - `run`, `interval_id`, `boat_name`, `opponent_name`\n",
    "     - `boat_role` (master/slave), `boat_weight`, `interval_duration`, `mast_brand`\n",
    "\n",
    "4. **Compute directional gains over time**\n",
    "   - For each timestamp, accumulates **Forward**, **Lateral**, and **VMG** gains via `compute_directional_gain(master_df, slave_df)` (from `report_fct.py`).\n",
    "\n",
    "5. **Recompute “line” values (we realized there has been some confusion between the lines in the data we received and decided to recompute them)**\n",
    "   - Reassigns `Line_C`, `Line_L`, `Line_R` by sorted magnitude into:\n",
    "     - `Line_C2` (largest), `Line_L2` (middle), `Line_R2` (smallest)\n",
    "   - Derives `side_line2 = Line_L2 + Line_R2` and `total_line2 = side_line2 + Line_C2`.\n",
    "\n",
    "6. **Concatenate all rows**\n",
    "   - Stacks all intervals and both boats into a single DataFrame and writes it to **`all_data.csv`**.\n",
    "\n",
    "## Output\n",
    "\n",
    "- **`all_data.csv`**: unified, row-level dataset across all runs, intervals, boats, and derived signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d66b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:40:37.846883Z",
     "iopub.status.busy": "2025-09-13T12:40:37.846883Z",
     "iopub.status.idle": "2025-09-13T12:40:40.533196Z",
     "shell.execute_reply": "2025-09-13T12:40:40.533196Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from cog_analysis import load_boat_data, analyze_session\n",
    "from report_fct import filter_interval, compute_directional_gain\n",
    "import numpy as np\n",
    "\n",
    "def build_csv_from_summary(summary_path, data_root, output_csv=\"all_data.csv\"):\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        summary = json.load(f)\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for run_entry in summary:\n",
    "        run_name = run_entry[\"run\"]\n",
    "        intervals = run_entry[\"intervals\"]\n",
    "\n",
    "        # Recherche du dossier de la run\n",
    "        run_path = None\n",
    "        for root, dirs, files in os.walk(data_root):\n",
    "            if os.path.basename(root) == run_name:\n",
    "                run_path = root\n",
    "                break\n",
    "\n",
    "        if not run_path:\n",
    "            print(f\"⚠️ Run folder not found for: {run_name}\")\n",
    "            continue\n",
    "\n",
    "        # Chargement des fichiers CSV\n",
    "        csv_files = [f for f in os.listdir(run_path) if f.endswith(\".csv\")]\n",
    "        if len(csv_files) != 2:\n",
    "            print(f\"⚠️ Skipping {run_name}: expected 2 CSVs, found {len(csv_files)}\")\n",
    "            continue\n",
    "\n",
    "        csv_paths = [os.path.join(run_path, f) for f in csv_files]\n",
    "\n",
    "        try:\n",
    "            df1, df2, name1, name2 = load_boat_data(csv_paths[0], csv_paths[1])\n",
    "            if df1.empty or df2.empty:\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CSVs for {run_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Traitement des intervalles\n",
    "        for i, interval in enumerate(intervals):\n",
    "            start, end = interval[\"start_time\"], interval[\"end_time\"]\n",
    "            if end - start < 30:\n",
    "                print(f\"⚠️ Skipping interval {i + 1} for {run_name}: duration < 30 seconds\")\n",
    "                continue\n",
    "\n",
    "            df1_clip = filter_interval(df1, start, end)\n",
    "            df2_clip = filter_interval(df2, start, end)\n",
    "            if df1_clip.empty or df2_clip.empty:\n",
    "                print(f\"⚠️ Skipping interval {i + 1} for {run_name}: no data in interval\")\n",
    "                continue\n",
    "\n",
    "            # Définir les rôles master/slave\n",
    "            master_df, slave_df = (df1_clip, df2_clip)\n",
    "            if not interval.get(\"boat1_master_leeward\", False):\n",
    "                master_df, slave_df = df2_clip, df1_clip\n",
    "\n",
    "            # Construction des données ligne par ligne\n",
    "            for df_clip, prefix, other_prefix in [(df1_clip, \"boat1\", \"boat2\"), (df2_clip, \"boat2\", \"boat1\")]:\n",
    "                df = df_clip.copy()\n",
    "                df[\"run\"] = run_name\n",
    "                df[\"interval_id\"] = i + 1\n",
    "                df[\"boat_name\"] = interval.get(f\"{prefix}_name\", \"\")\n",
    "                df[\"opponent_name\"] = interval.get(f\"{other_prefix}_name\", \"\")\n",
    "                df[\"boat_role\"] = \"master\" if interval.get(f\"{prefix}_master_leeward\", False) else \"slave\"\n",
    "                df[\"boat_weight\"] = interval.get(f\"{prefix}_total_weight\", None)\n",
    "                df[\"interval_duration\"] = interval.get(\"duration\", None)\n",
    "                df[\"mast_brand\"] = interval.get(f\"{prefix}_mast_brand\", None)\n",
    "\n",
    "                # Calculs des gains\n",
    "                gain_forward = []\n",
    "                gain_lateral = []\n",
    "                gain_vmg = []\n",
    "\n",
    "                for t in df[\"SecondsSince1970\"]:\n",
    "                    m_clip = master_df[master_df[\"SecondsSince1970\"] <= t]\n",
    "                    s_clip = slave_df[slave_df[\"SecondsSince1970\"] <= t]\n",
    "                    gain_df = compute_directional_gain(m_clip, s_clip)\n",
    "\n",
    "                    if gain_df.empty:\n",
    "                        gain_forward.append(np.nan)\n",
    "                        gain_lateral.append(np.nan)\n",
    "                        gain_vmg.append(np.nan)\n",
    "                    else:\n",
    "                        gain_forward.append(gain_df.loc[\"Total Gain\", \"Forward\"])\n",
    "                        gain_lateral.append(gain_df.loc[\"Total Gain\", \"Lateral\"])\n",
    "                        gain_vmg.append(gain_df.loc[\"Total Gain\", \"VMG\"])\n",
    "\n",
    "                df[\"gain_forward\"] = gain_forward\n",
    "                df[\"gain_lateral\"] = gain_lateral\n",
    "                df[\"gain_vmg\"] = gain_vmg\n",
    "                \n",
    "                # Réaffectation arbitraire des lignes en triant les valeurs\n",
    "                lines = df[[\"Line_C\", \"Line_L\", \"Line_R\"]].values\n",
    "                sorted_lines = np.sort(lines, axis=1)  # tri croissant ligne par ligne\n",
    "\n",
    "                # Création des nouvelles colonnes avec attribution arbitraire\n",
    "                df[\"Line_R2\"] = sorted_lines[:, 0]  # plus petit\n",
    "                df[\"Line_L2\"] = sorted_lines[:, 1]  # milieu\n",
    "                df[\"Line_C2\"] = sorted_lines[:, 2]  # plus grand\n",
    "                df[\"side_line2\"] = df[\"Line_L2\"] + df[\"Line_R2\"]\n",
    "                df[\"total_line2\"] = df[\"side_line2\"] + df[\"Line_C2\"]\n",
    "\n",
    "                all_rows.append(df)\n",
    "\n",
    "    # Sauvegarde finale\n",
    "    if not all_rows:\n",
    "        print(\"❌ No valid data found.\")\n",
    "        return\n",
    "\n",
    "    df_global = pd.concat(all_rows, ignore_index=True)\n",
    "    df_global.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Global CSV saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dbb311c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:40:40.533196Z",
     "iopub.status.busy": "2025-09-13T12:40:40.533196Z",
     "iopub.status.idle": "2025-09-13T12:42:58.766676Z",
     "shell.execute_reply": "2025-09-13T12:42:58.765675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Global CSV saved to: all_data.csv\n"
     ]
    }
   ],
   "source": [
    "build_csv_from_summary(\n",
    "    summary_path=\"summary_enriched.json\",\n",
    "    data_root=\"../Data_Sailnjord/Straight_lines\",\n",
    "    output_csv=\"all_data.csv\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sail2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
